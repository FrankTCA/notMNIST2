{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n",
      "Initialized\n",
      "Loss at step 0: 21.120708\n",
      "Training accuracy: 6.6%\n",
      "Validation accuracy: 9.2%\n",
      "Test accuracy: 9.1%\n",
      "Loss at step 100: 2.318366\n",
      "Training accuracy: 71.4%\n",
      "Validation accuracy: 70.3%\n",
      "Test accuracy: 77.6%\n",
      "Loss at step 200: 1.872368\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 72.7%\n",
      "Test accuracy: 80.0%\n",
      "Loss at step 300: 1.630490\n",
      "Training accuracy: 75.6%\n",
      "Validation accuracy: 73.3%\n",
      "Test accuracy: 80.8%\n",
      "Loss at step 400: 1.466963\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 73.8%\n",
      "Test accuracy: 81.4%\n",
      "Loss at step 500: 1.345487\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 74.3%\n",
      "Test accuracy: 81.8%\n",
      "Loss at step 600: 1.249798\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 74.6%\n",
      "Test accuracy: 82.0%\n",
      "Loss at step 700: 1.171663\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 74.8%\n",
      "Test accuracy: 82.1%\n",
      "Loss at step 800: 1.106275\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 74.9%\n",
      "Test accuracy: 82.2%\n",
      "Initialized\n",
      "Minibatch loss at step 1: 14.735652\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy 16.1%\n",
      "Test accuracy: 17.7%\n",
      "Minibatch loss at step 501: 1.532573\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy 76.2%\n",
      "Test accuracy: 83.6%\n",
      "Minibatch loss at step 1001: 1.287615\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy 77.0%\n",
      "Test accuracy: 84.5%\n",
      "Minibatch loss at step 1501: 1.061449\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy 77.6%\n",
      "Test accuracy: 84.8%\n",
      "Minibatch loss at step 2001: 1.045676\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy 78.1%\n",
      "Test accuracy: 85.1%\n",
      "Minibatch loss at step 2501: 1.030679\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy 78.5%\n",
      "Test accuracy: 85.5%\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save #free up some memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0, ...], 1 to [0.0, 1.0, 0.0, ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data\n",
    "    #load datasets into graph\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables\n",
    "    # Parameters for training\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation\n",
    "    # Will multiply wieghts and add biases.\n",
    "    # Compute softmax and cross-entropy\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # Optimizer\n",
    "    # Uses gradient descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "    \n",
    "    num_steps = 801\n",
    "    \n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # Ensure parameters get initialized\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        \n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(predictions, train_labels[:train_subset, :]))\n",
    "            \n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            \n",
    "            print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "    \n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 1):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
